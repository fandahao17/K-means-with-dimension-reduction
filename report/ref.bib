@incollection{alelyani2018feature,
  title={Feature selection for clustering: A review},
  author={Alelyani, Salem and Tang, Jiliang and Liu, Huan},
  booktitle={Data Clustering},
  pages={29--60},
  year={2018},
  publisher={Chapman and Hall/CRC}
}

@article{boutsidis2014randomized,
  title={Randomized dimensionality reduction for $ k $-means clustering},
  author={Boutsidis, Christos and Zouzias, Anastasios and Mahoney, Michael W and Drineas, Petros},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={2},
  pages={1045--1062},
  year={2014},
  publisher={IEEE}
}

@inproceedings{rudi2018fast,
  title={On fast leverage score sampling and optimal learning},
  author={Rudi, Alessandro and Calandriello, Daniele and Carratino, Luigi and Rosasco, Lorenzo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5672--5682},
  year={2018}
}

@article{drineas2012fast,
  title={Fast approximation of matrix coherence and statistical leverage},
  author={Drineas, Petros and Magdon-Ismail, Malik and Mahoney, Michael W and Woodruff, David P},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={Dec},
  pages={3475--3506},
  year={2012}
}

@article{szlam2014implementation,
  title={An implementation of a randomized algorithm for principal component analysis},
  author={Szlam, Arthur and Kluger, Yuval and Tygert, Mark},
  journal={arXiv preprint arXiv:1412.3510},
  year={2014}
}

@article{halko2011finding,
  title={Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions},
  author={Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A},
  journal={SIAM review},
  volume={53},
  number={2},
  pages={217--288},
  year={2011},
  publisher={SIAM}
}

@inproceedings{drineas1999clustering,
  title={Clustering in Large Graphs and Matrices.},
  author={Drineas, Petros and Frieze, Alan M and Kannan, Ravi and Vempala, Santosh and Vinay, V},
  booktitle={SODA},
  volume={99},
  pages={291--299},
  year={1999},
  organization={Citeseer}
}

@book{strang2019linear,
  title={Linear algebra and learning from data},
  author={Strang, Gilbert},
  year={2019},
  publisher={Wellesley-Cambridge Press}
}

@inproceedings{boutsidis2009unsupervised,
  title={Unsupervised feature selection for the $ k $-means clustering problem},
  author={Boutsidis, Christos and Drineas, Petros and Mahoney, Michael W},
  booktitle={Advances in Neural Information Processing Systems},
  pages={153--161},
  year={2009}
}

@book{blum2020foundations,
  title={Foundations of data science},
  author={Blum, Avrim and Hopcroft, John and Kannan, Ravindran},
  year={2020},
  publisher={Cambridge University Press}
}

@article{boutsidis2013deterministic,
  title={Deterministic feature selection for k-means clustering},
  author={Boutsidis, Christos and Magdon-Ismail, Malik},
  journal={IEEE Transactions on Information Theory},
  volume={59},
  number={9},
  pages={6099--6110},
  year={2013},
  publisher={IEEE}
}

@inproceedings{10.5555/2997189.2997223,
author = {Boutsidis, Christos and Zouzias, Anastasios and Drineas, Petros},
title = {Random Projections for K-Means Clustering},
year = {2010},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1},
pages = {298–306},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS’10}
}

@article{10.1145/1646353.1646379,
author = {Ailon, Nir and Chazelle, Bernard},
title = {Faster Dimension Reduction},
year = {2010},
issue_date = {February 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/1646353.1646379},
doi = {10.1145/1646353.1646379},
journal = {Commun. ACM},
month = feb,
pages = {97–104},
numpages = {8}
}

@article{10.1016/j.ipl.2008.09.028,
author = {Liberty, Edo and Zucker, Steven W.},
title = {The Mailman Algorithm: A Note on Matrix--Vector Multiplication},
year = {2009},
issue_date = {January, 2009},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {109},
number = {3},
issn = {0020-0190},
url = {https://doi.org/10.1016/j.ipl.2008.09.028},
doi = {10.1016/j.ipl.2008.09.028},
journal = {Inf. Process. Lett.},
month = jan,
pages = {179–182},
numpages = {4},
keywords = {Matrix--vector multiplication, Algorithms, Mailman algorithm, Random projections}
}
@article {Mahoney2009cur,
	author = {Mahoney, Michael W. and Drineas, Petros},
	title = {CUR matrix decompositions for improved data analysis},
	volume = {106},
	number = {3},
	pages = {697--702},
	year = {2009},
	doi = {10.1073/pnas.0803205106},
	publisher = {National Academy of Sciences},
	abstract = {Principal components analysis and, more generally, the Singular Value Decomposition are fundamental data analysis tools that express a data matrix in terms of a sequence of orthogonal or uncorrelated vectors of decreasing importance. Unfortunately, being linear combinations of up to all the data points, these vectors are notoriously difficult to interpret in terms of the data and processes generating the data. In this article, we develop CUR matrix decompositions for improved data analysis. CUR decompositions are low-rank matrix decompositions that are explicitly expressed in terms of a small number of actual columns and/or actual rows of the data matrix. Because they are constructed from actual data elements, CUR decompositions are interpretable by practitioners of the field from which the data are drawn (to the extent that the original data are). We present an algorithm that preferentially chooses columns and rows that exhibit high {\textquotedblleft}statistical leverage{\textquotedblright} and, thus, in a very precise statistical sense, exert a disproportionately large {\textquotedblleft}influence{\textquotedblright} on the best low-rank fit of the data matrix. By selecting columns and rows in this manner, we obtain improved relative-error and constant-factor approximation guarantees in worst-case analysis, as opposed to the much coarser additive-error guarantees of prior work. In addition, since the construction involves computing quantities with a natural and widely studied statistical interpretation, we can leverage ideas from diagnostic regression analysis to employ these matrix decompositions for exploratory data analysis.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/106/3/697},
	eprint = {https://www.pnas.org/content/106/3/697.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{johnson1984extensions,
  title={Extensions of Lipschitz mappings into a Hilbert space},
  author={Johnson, William B and Lindenstrauss, Joram},
  journal={Contemporary mathematics},
  volume={26},
  number={189-206},
  pages={1},
  year={1984}
}

@article{rudelson2007sampling,
  title={Sampling from large matrices: An approach through geometric functional analysis},
  author={Rudelson, Mark and Vershynin, Roman},
  journal={Journal of the ACM (JACM)},
  volume={54},
  number={4},
  pages={21--es},
  year={2007},
  publisher={ACM New York, NY, USA}
}

@article{chechik2007eec,
  title={{Euclidean Embedding of Co-occurrence Data}},
  author={Globerson, A. and Chechik, G. and Pereira, F. and Tishby, N.},
  journal={The Journal of Machine Learning Research},
  volume={8},
  pages={2265--2295},
  year={2007},
  publisher={MIT Press Cambridge, MA, USA}
}

@article{maaten2008visualizing,
  title={Visualizing data using t-SNE},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}